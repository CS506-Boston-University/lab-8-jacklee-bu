{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸŽ¬ IMDB Movie Review Sentiment Dataset\n",
    "\n",
    "### ðŸ§¾ Overview\n",
    "\n",
    "The **IMDB Movie Review Dataset** is a widely used benchmark in Natural Language Processing (NLP) for **binary sentiment classification** â€” determining whether a movie review expresses a **positive** or **negative** opinion.\n",
    "\n",
    "Each sample in the dataset is a single movie review written by a user on the [Internet Movie Database (IMDB)](https://www.imdb.com/), labeled as:\n",
    "\n",
    "- **Positive (1):** The review reflects enjoyment or approval of the movie.\n",
    "- **Negative (0):** The review expresses dislike or criticism of the movie.\n",
    "\n",
    "The dataset contains:\n",
    "- **25,000** labeled training reviews  \n",
    "- **25,000** labeled test reviews  \n",
    "All text data is preprocessed, and each review is already tokenized into words or indices (depending on the version you load).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“¦ Dataset Source\n",
    "\n",
    "This dataset is included in several libraries:\n",
    "\n",
    "- **Keras / TensorFlow:**  \n",
    "  ```python\n",
    "  from tensorflow.keras.datasets import imdb\n",
    "  (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)"
   ],
   "metadata": {
    "id": "hpmKGS3RWsii"
   },
   "id": "hpmKGS3RWsii"
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# If needed:\n",
    "# %pip install datasets --quiet\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_imdb_via_hf(subset_train=20000, subset_test=5000, seed=7):\n",
    "    \"\"\"Load IMDB via Hugging Face datasets, returning raw text and labels.\n",
    "    Subsample to keep runtime manageable.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        ds = load_dataset(\"imdb\")\n",
    "        # Convert to pandas for easier subsampling\n",
    "        train_df = pd.DataFrame({'text': ds['train']['text'], 'label': ds['train']['label']})\n",
    "        test_df  = pd.DataFrame({'text': ds['test']['text'],  'label': ds['test']['label']})\n",
    "        # Subsample\n",
    "        train_df = train_df.sample(n=min(subset_train, len(train_df)), random_state=seed).reset_index(drop=True)\n",
    "        test_df  = test_df.sample(n=min(subset_test,  len(test_df)),  random_state=seed).reset_index(drop=True)\n",
    "        return train_df['text'].tolist(), train_df['label'].to_numpy(), test_df['text'].tolist(), test_df['label'].to_numpy()\n",
    "    except Exception as e:\n",
    "        print(\"HF datasets not available or failed:\", e)\n",
    "        return None\n",
    "\n",
    "def load_imdb_via_keras(subset_train=20000, subset_test=5000, seed=7):\n",
    "    \"\"\"Fallback loader using Keras integer-encoded IMDB; returns decoded text strings if possible.\"\"\"\n",
    "    try:\n",
    "        # %pip install tensorflow --quiet\n",
    "        from tensorflow.keras.datasets import imdb\n",
    "        from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "        from tensorflow.keras.datasets.imdb import get_word_index\n",
    "\n",
    "        num_words = 50000\n",
    "        (X_tr_i, y_tr), (X_te_i, y_te) = imdb.load_data(num_words=num_words)\n",
    "        # Pad and decode into crude space-joined tokens\n",
    "        word_index = get_word_index()\n",
    "        index_word = {v+3: k for k,v in word_index.items()}\n",
    "        index_word[0] = \"<pad>\"; index_word[1] = \"<start>\"; index_word[2] = \"<unk>\"; index_word[3] = \"<unused>\"\n",
    "        def decode(seq):\n",
    "            return \" \".join(index_word.get(i, \"<unk>\") for i in seq)\n",
    "\n",
    "        # Subsample\n",
    "        rng = np.random.default_rng(seed)\n",
    "        tr_idx = rng.choice(len(X_tr_i), size=min(subset_train, len(X_tr_i)), replace=False)\n",
    "        te_idx = rng.choice(len(X_te_i), size=min(subset_test,  len(X_te_i)), replace=False)\n",
    "        X_tr_txt = [decode(X_tr_i[i]) for i in tr_idx]\n",
    "        X_te_txt = [decode(X_te_i[i]) for i in te_idx]\n",
    "        y_tr_s = y_tr[tr_idx]; y_te_s = y_te[te_idx]\n",
    "        return X_tr_txt, y_tr_s, X_te_txt, y_te_s\n",
    "    except Exception as e:\n",
    "        print(\"Keras IMDB not available or failed:\", e)\n",
    "        return None\n",
    "\n",
    "# Try HF first, then Keras fallback\n",
    "data = load_imdb_via_hf(subset_train=20000, subset_test=5000, seed=7)\n",
    "if data is None:\n",
    "    data = load_imdb_via_keras(subset_train=20000, subset_test=5000, seed=7)\n",
    "\n",
    "if data is None:\n",
    "    raise RuntimeError(\"Could not load IMDB via Hugging Face or Keras. Install one of them and retry.\")\n",
    "\n",
    "X_train_text, y_train, X_test_text, y_test = data\n",
    "len(X_train_text), len(X_test_text), np.bincount(y_train), np.bincount(y_test)\n"
   ],
   "metadata": {
    "id": "WlJ1hpLeRuZh"
   },
   "id": "WlJ1hpLeRuZh",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(X_train_text[0][:100])"
   ],
   "metadata": {
    "id": "UPdexDMPWS69"
   },
   "id": "UPdexDMPWS69",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(np.unique(y_train))"
   ],
   "metadata": {
    "id": "GngfwaDoWLTx"
   },
   "id": "GngfwaDoWLTx",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Get the unique values and their counts\n",
    "unique_values, counts = np.unique(y_test, return_counts=True)\n",
    "\n",
    "# The number of unique values is simply the length of the unique_values array\n",
    "num_unique_values = len(y_test)\n",
    "\n",
    "print(f\"Unique values: {y_test}\")\n",
    "print(f\"Counts of unique values: {counts}\")\n",
    "print(f\"Number of unique values: {num_unique_values}\")"
   ],
   "metadata": {
    "id": "8ntem9MOf8dx"
   },
   "id": "8ntem9MOf8dx",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    max_features=50000,\n",
    "    ngram_range=(1,2),\n",
    "    min_df=2\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "id": "H5XNb0bBR6Y-"
   },
   "id": "H5XNb0bBR6Y-",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "69876f86",
   "metadata": {
    "id": "69876f86"
   },
   "source": [
    "\n",
    "### Fromâ€‘Scratch Linear SVM on TruncatedSVD Features\n",
    "\n",
    "Our NumPyâ€‘only `LinearSVM` expects dense features. Weâ€™ll reuse the SVDâ€‘reduced dense representation (200â€‘D) and train the fromâ€‘scratch model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef4e536",
   "metadata": {
    "id": "6ef4e536"
   },
   "source": [
    "\n",
    "### ðŸ“š Math for this section â€” TFâ€‘IDF â†’ SVD (LSA) â†’ Linear SVM\n",
    "\n",
    "**TFâ€‘IDF weighting** for term \\(t\\) in document \\(d\\):\n",
    "$\n",
    "\\mathrm{tf\\!-\\!idf}(t,d) = \\mathrm{tf}(t,d)\\cdot \\underbrace{\\log\\frac{N}{\\mathrm{df}(t)+1}}_{\\mathrm{idf}(t)},\n",
    "$\n",
    "where $(\\mathrm{tf}(t,d)$) is term frequency, $(\\mathrm{df}(t)$) is document frequency, and \\(N\\) is #docs.\n",
    "\n",
    "**Truncated SVD (LSA)** on sparse documentâ€“term matrix $(X) (size (N\\times V$)):\n",
    "$\n",
    "X \\approx U_k \\Sigma_k V_k^\\top,\\quad \\text{project } \\mathbf{z}=\\mathbf{x}V_k \\in \\mathbb{R}^k.\n",
    "$\n",
    "\n",
    "**Standardization** per feature $(j): (\\tilde{z}_j = (z_j-\\mu_j)/\\sigma_j)$.\n",
    "\n",
    "**Linear SVM hinge objective (binary)** with $(y_i'\\in\\{-1,+1\\}$):\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w},b}\\; \\tfrac{1}{2}\\lVert \\mathbf{w}\\rVert^2 + C\\sum_i \\max(0,\\,1 - y_i'(\\mathbf{w}^\\top\\mathbf{x}_i+b)).\n",
    "$$\n",
    "\n",
    "**Subgradient updates** (per sample) with margin $(m_i=y_i'(\\mathbf{w}^\\top\\mathbf{x}_i+b)$):\n",
    "$\n",
    "\\begin{aligned}\n",
    "m_i \\ge 1 &:&& \\mathbf{w}\\leftarrow \\mathbf{w}-\\eta\\,\\mathbf{w} \\\\\n",
    "m_i < 1   &:&& \\mathbf{w}\\leftarrow \\mathbf{w}-\\eta\\,(\\mathbf{w}-C\\,y_i'\\mathbf{x}_i),\\;\\; b\\leftarrow b+\\eta\\,C\\,y_i'.\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "**Linear decision function:**\n",
    "$s =  \\mathbf{w}^\\top\\mathbf{x}_i+b$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3a49b2",
   "metadata": {
    "id": "6e3a49b2"
   },
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report\n\nclass LinearSVMScratch:\n    def __init__(self, C=1.0, lr=0.1, n_epochs=10, lr_decay=0.0, shuffle=True, random_state=7, early_stopping=False, es_patience=3):\n        self.C = float(C)\n        self.lr = float(lr)\n        self.n_epochs = int(n_epochs)\n        self.lr_decay = float(lr_decay)\n        self.shuffle = bool(shuffle)\n        self.random_state = int(random_state)\n        self.early_stopping = bool(early_stopping)\n        self.es_patience = int(es_patience)\n        self.w_ = None\n        self.b_ = 0.0\n        self.loss_curve_ = []\n\n    @staticmethod\n    def _to_pm1(y):\n        y = np.asarray(y).astype(int)\n        if set(np.unique(y)) == {0,1}:\n            return np.where(y==1, 1, -1)\n        elif set(np.unique(y)) == {-1,1}:\n            return y\n        else:\n            raise ValueError(\"Labels must be in {0,1} or {-1,+1}.\")\n\n    def _hinge_loss(self, X, ypm1):\n        margins = 1.0 - ypm1 * (X @ self.w_ + self.b_)\n        return 0.5*np.dot(self.w_, self.w_) + self.C * np.maximum(0.0, margins).sum()\n\n    def fit(self, X, y):\n        rng = np.random.default_rng(self.random_state)\n        X = np.asarray(X, dtype=float)\n        ypm1 = self._to_pm1(y)\n        n, d = X.shape\n        self.w_ = np.zeros(d, dtype=float)\n        self.b_ = 0.0\n        idx = np.arange(n)\n        best = np.inf\n        bad = 0\n\n        for epoch in range(self.n_epochs):\n            if self.shuffle:\n                rng.shuffle(idx)\n            eta = self.lr / (1.0 + self.lr_decay * epoch)\n\n            for i in idx:\n                xi = X[i]; yi = ypm1[i]\n                margin = yi * (np.dot(self.w_, xi) + self.b_)\n                if margin >= 1.0:\n                    self.w_ -= eta * self.w_\n                else:\n                    self.w_ -= eta * (self.w_ - self.C * yi * xi)\n                    self.b_ += eta * self.C * yi\n\n            loss = self._hinge_loss(X, ypm1)\n            self.loss_curve_.append(float(loss))\n            print(f\"Epoch {epoch+1:02d}/{self.n_epochs}  hinge+L2 loss: {loss:.3f}\")\n\n            if self.early_stopping:\n                if loss < best - 1e-4:\n                    best = loss; bad = 0\n                else:\n                    bad += 1\n                    if bad >= self.es_patience:\n                        print(\"Early stopping.\")\n                        break\n\n        return self\n\n    def decision_function(self, X):\n        return X @ self.w_ + self.b_\n\n    def predict(self, X):\n        s = self.decision_function(X)\n        return (s >= 0.0).astype(int)\n\nvectorizer = TfidfVectorizer(stop_words='english', max_features=60000, ngram_range=(1,2), min_df=2)\nsvd = TruncatedSVD(n_components=200, random_state=7)\nscaler = StandardScaler(with_mean=True, with_std=True)\n\nXtr_sparse = vectorizer.fit_transform(X_train_text)\nXte_sparse = vectorizer.transform(X_test_text)\n\nXtr_red = svd.fit_transform(Xtr_sparse)\nXte_red = svd.transform(Xte_sparse)\n\nXtr = scaler.fit_transform(Xtr_red)\nXte = scaler.transform(Xte_red)\n\nfor lr_val in [0.1,0.05,0.01,0.001]:\n  svm = LinearSVMScratch(C=1.0, lr=lr_val, n_epochs=10, lr_decay=0.02, early_stopping=True, es_patience=2, random_state=7)\n  svm.fit(Xtr, y_train)\n\n  y_pred = svm.predict(Xtr)\n  print(\"From-scratch Linear SVM â€” IMDB (SVD-200)\")\n  print(\"Train accuracy:\", accuracy_score(y_train, y_pred))\n  print(classification_report(y_train, y_pred, target_names=['neg','pos'], digits=3))\n\n  plt.figure()\n  plt.plot(svm.loss_curve_)\n  plt.title(\"Hinge Loss (with L2) per Epoch â€” From-scratch Linear SVM\")\n  plt.xlabel(\"Epoch\")\n  plt.ylabel(\"Loss\")\n  plt.show()"
  },
  {
   "cell_type": "code",
   "source": [
    "svm = LinearSVMScratch(C=1.0, lr=lr_val, n_epochs=10, lr_decay=0.02, early_stopping=True, es_patience=2, random_state=7)\n",
    "svm.fit(Xtr, y_train)\n",
    "\n",
    "y_pred = svm.predict(Xte)\n",
    "print(\"From-scratch Linear SVM â€” IMDB (SVD-200)\")\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=['neg','pos'], digits=3))\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure()\n",
    "plt.plot(svm.loss_curve_)\n",
    "plt.title(\"Hinge Loss (with L2) per Epoch â€” From-scratch Linear SVM\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "WBE5RMfOdIv5"
   },
   "id": "WBE5RMfOdIv5",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from sklearn import svm\nfrom sklearn.metrics import accuracy_score\n\nlin = svm.LinearSVC(C=1, max_iter=2000, dual=True, random_state=7)\nlin.fit(Xtr, y_train)\npred_lin = lin.predict(Xte)\n\nprint(\"LinearSVC â€” test accuracy:\", accuracy_score(y_test, pred_lin))",
   "metadata": {
    "id": "WmqcneLdtTA8"
   },
   "id": "WmqcneLdtTA8",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ§  Mathematical Foundations of the Polynomial Kernel SVM\n",
    "\n",
    "### 1. Problem Setup\n",
    "\n",
    "We are given training data  \n",
    "$\n",
    "\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n, \\quad \\mathbf{x}_i \\in \\mathbb{R}^d, \\; y_i \\in \\{-1, +1\\}.\n",
    "$\n",
    "\n",
    "The goal of a Support Vector Machine (SVM) is to find a decision function  \n",
    "**Decision Function**: $\n",
    "f(\\mathbf{x}) = \\sum_{j=1}^n \\beta_j K(\\mathbf{x}_j, \\mathbf{x}) + b\n",
    "$\n",
    "\n",
    "that separates the two classes with the largest possible margin.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. The Polynomial Kernel\n",
    "\n",
    "Instead of using raw dot products, we map data into a higher-dimensional space via the **polynomial kernel**:\n",
    "$\n",
    "K(\\mathbf{x}, \\mathbf{z}) = (G\\, \\mathbf{x}^\\top \\mathbf{z} + c_0)^{d},\n",
    "$\n",
    "where:\n",
    "\n",
    "- $( G > 0 )$ controls how strongly similarity depends on the dot product  \n",
    "- $( c_0 )$ (often called `coef0` in code) shifts the kernel to include lower-order terms  \n",
    "- $( d )$ is the **degree** of the polynomial and controls nonlinearity\n",
    "\n",
    "This kernel implicitly represents all polynomial interactions between features up to degree $(d)$.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Regularized Risk (Squared Hinge Form)\n",
    "\n",
    "We minimize the **regularized empirical risk** in the Reproducing Kernel Hilbert Space (RKHS):\n",
    "\n",
    "$\n",
    "J(\\boldsymbol{\\beta}, b) = \\frac{\\lambda}{2}\\,\\boldsymbol{\\beta}^\\top K \\boldsymbol{\\beta} + \\frac{1}{n}\\sum_{i=1}^n \\max(0,\\, 1 - y_i f_i)^2, \\text{where} \\quad f_i = (K\\boldsymbol{\\beta})_i + b \\quad \\text{and} \\quad K_{ij} = K(\\mathbf{x}_i, \\mathbf{x}_j). $\n",
    "\n",
    "- The first term $( \\frac{\\lambda}{2}\\boldsymbol{\\beta}^\\top K \\boldsymbol{\\beta} )$ is a **regularizer** that penalizes model complexity.  \n",
    "- The second term is the **squared hinge loss**, which penalizes samples that violate the margin constraint $( y_i f_i \\ge 1 )$.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Gradient Derivations\n",
    "\n",
    "Let $( r_i = \\max(0, 1 - y_i f_i) )$.  \n",
    "Only samples with $( y_i f_i < 1 )$ contribute to the gradient.\n",
    "\n",
    "**Gradients:**\n",
    "\n",
    "$ \\nabla_{\\boldsymbol{\\beta}} J = \\lambda K \\boldsymbol{\\beta} - \\frac{2}{n}\\, K(\\mathbf{y} \\odot \\mathbf{r}), \\nabla_b J = -\\frac{2}{n}\\sum_{i=1}^n y_i r_i.\n",
    "$\n",
    "\n",
    "Here $( \\odot )$ denotes element-wise multiplication.\n",
    "\n",
    "These are the update directions used by the optimizer in the implementation.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Optimization via Gradient Descent\n",
    "\n",
    "We update parameters using gradient-based optimization (Adam in the code):\n",
    "\n",
    "$\n",
    "\\boldsymbol{\\beta} \\leftarrow \\boldsymbol{\\beta} - \\eta\\, \\nabla_{\\boldsymbol{\\beta}} J,\n",
    "\\quad\n",
    "b \\leftarrow b - \\eta\\, \\nabla_b J.\n",
    "$\n",
    "\n",
    "Adam adaptively rescales these gradients using running estimates of their first and second moments.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Decision Function After Training\n",
    "\n",
    "The learned decision function becomes:\n",
    "\n",
    "$\n",
    "f(\\mathbf{x}) = \\sum_{j=1}^n \\beta_j K(\\mathbf{x}_j, \\mathbf{x}) + b,\n",
    "$\n",
    "and the prediction rule is:\n",
    "$\n",
    "\\hat{y} = \\operatorname{sign}(f(\\mathbf{x})).\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Interpretation\n",
    "\n",
    "| Symbol | Meaning | Notes |\n",
    "|:-------|:---------|:------|\n",
    "| $(K(\\mathbf{x}, \\mathbf{z})$) | Polynomial kernel | Expands dot products to polynomial feature space |\n",
    "| $(\\boldsymbol{\\beta}$) | Coefficients of the kernel expansion | Similar to dual variables \\( \\alpha \\) in classic SVM |\n",
    "| $(b$) | Bias/intercept | Shifts the decision boundary |\n",
    "| $(\\lambda$) | Regularization strength | Controls smoothness of boundary |\n",
    "| $(r_i$) | Margin violation residual | Non-zero if $( y_i f_i < 1 $) |\n",
    "\n",
    "The polynomial kernel allows the SVM to form **curved, nonlinear decision boundaries**.  \n",
    "Higher degrees \\(d\\) increase flexibility, but also the risk of overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Summary of the Objective\n",
    "\n",
    "The optimization performed in code minimizes:\n",
    "$ \\boxed{ \\min_{\\boldsymbol{\\beta}, b} \\left[ \\frac{\\lambda}{2}\\boldsymbol{\\beta}^\\top K\\boldsymbol{\\beta} + \\frac{1}{n}\\sum_{i=1}^n \\max(0, 1 - y_i((K\\boldsymbol{\\beta})_i + b))^2 \\right] }\n",
    "$\n",
    "\n",
    "This is a smooth, differentiable version of the kernel SVM objective that can be solved by gradient descent without SMO or quadratic programming.\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "PqtSuCst2y1w"
   },
   "id": "PqtSuCst2y1w"
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom time import time\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import make_moons, make_circles\nfrom sklearn.model_selection import train_test_split\n\nclass PolyKernelSVM_GD:\n    \"\"\"\n    Kernel SVM via gradient descent (squared hinge in a kernelized primal).\n    J(Î², b) = (Î»/2) * Î²^T K Î² + (1/n) * Î£_i max(0, 1 - y_i * (KÎ² + b)_i)^2\n    Decision: f(x) = Î£_j Î²_j K(x_j, x) + b\n    Labels in {-1, +1}.\n    \"\"\"\n    def __init__(self, degree=3, gamma=None, coef0=1.0, lam=1e-2,\n                 lr=5e-2, epochs=2000, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-8,\n                 random_state=0, verbose=False):\n        self.degree = int(degree)\n        self.gamma = (1.0) if gamma is None else float(gamma)\n        self.coef0 = float(coef0)\n        self.lam = float(lam)\n        self.lr = float(lr)\n        self.epochs = int(epochs)\n        self.beta1 = float(adam_beta1)\n        self.beta2 = float(adam_beta2)\n        self.adam_eps = float(adam_eps)\n        self.rs = np.random.RandomState(random_state)\n        self.verbose = verbose\n        self.X = None; self.beta = None; self.b = 0.0; self.K = None\n\n    def _poly_kernel(self, X, Z):\n        X = np.asarray(X, dtype=float); Z = np.asarray(Z, dtype=float)\n        g = self.gamma if self.gamma is not None else 1.0 / X.shape[1]\n        return (g * (X @ Z.T) + self.coef0) ** self.degree\n\n    def fit(self, X, y):\n        X = np.asarray(X, dtype=float)\n        y = (np.asarray(y, dtype=float))\n        assert set(np.unique(y)).issubset({-1.0, 1.0}), \"Labels must be {-1,+1}\"\n        n = X.shape[0]\n        self.X = X\n        self.K = self._poly_kernel(X, X)\n        self.beta = np.zeros(n); self.b = 0.0\n\n        m_beta = np.zeros_like(self.beta); v_beta = np.zeros_like(self.beta)\n        m_b = 0.0; v_b = 0.0\n\n        for t in range(1, self.epochs + 1):\n            f = self.K @ self.beta + self.b\n            margin = y * f\n            r = np.clip(1.0 - margin, 0.0, None)\n\n            grad_beta = self.lam * (self.K @ self.beta) - (2.0 / n) * (self.K @ (y * r))\n            grad_b = -(2.0 / n) * np.sum(y * r)\n\n            m_beta = self.beta1*m_beta + (1-self.beta1)*grad_beta\n            v_beta = self.beta2*v_beta + (1-self.beta2)*(grad_beta**2)\n            m_b = self.beta1*m_b + (1-self.beta1)*grad_b\n            v_b = self.beta2*v_b + (1-self.beta2)*(grad_b**2)\n\n            m_beta_hat = m_beta / (1 - self.beta1**t)\n            v_beta_hat = v_beta / (1 - self.beta2**t)\n            m_b_hat = m_b / (1 - self.beta1**t)\n            v_b_hat = v_b / (1 - self.beta2**t)\n\n            self.beta -= self.lr * m_beta_hat / (np.sqrt(v_beta_hat) + self.adam_eps)\n            self.b    -= self.lr * m_b_hat / (np.sqrt(v_b_hat) + self.adam_eps)\n\n            if self.verbose and (t % max(1, self.epochs // 10) == 0 or t == 1):\n                reg = 0.5 * self.lam * (self.beta @ (self.K @ self.beta))\n                loss = (r ** 2).mean()\n                print(f\"[{t:5d}/{self.epochs}] loss={loss:.4f} reg={reg:.4f} obj={loss+reg:.4f}\")\n        return self\n\n    def decision_function(self, Xq):\n        Kq = self._poly_kernel(np.asarray(Xq, dtype=float), self.X)\n        return Kq @ self.beta + self.b\n\n    def predict(self, Xq):\n        return np.where(self.decision_function(Xq) >= 0.0, 1.0, -1.0)\n\ndef plot_side_by_side(models, titles, X, y):\n    x_min, x_max = X[:,0].min()-0.8, X[:,0].max()+0.8\n    y_min, y_max = X[:,1].min()-0.8, X[:,1].max()+0.8\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n                         np.linspace(y_min, y_max, 400))\n    grid = np.c_[xx.ravel(), yy.ravel()]\n    fig, axes = plt.subplots(1, len(models), figsize=(6*len(models), 5))\n    if len(models) == 1: axes = [axes]\n    for ax, model, title in zip(axes, models, titles):\n        zz = model.decision_function(grid).reshape(xx.shape)\n        ax.contourf(xx, yy, np.sign(zz), levels=[-np.inf,0,np.inf], alpha=0.2)\n        cs = ax.contour(xx, yy, zz, levels=[-1,0,1], linestyles=['--','-','--'])\n        ax.clabel(cs, inline=True, fontsize=8)\n        ax.scatter(X[:,0], X[:,1], c=(y>0).astype(int), cmap='bwr', edgecolor='k', s=30)\n        ax.set_title(title); ax.set_xlabel(\"x1\"); ax.set_ylabel(\"x2\")\n    plt.tight_layout(); plt.show()\n\nDATASET = \"moons\"\nNOISE = 0.25\nN_SAMPLES = 600\nTEST_SIZE = 0.3\nRS = 42\n\nDEGREE = 3\nGAMMA = 1.0\nCOEF0 = 1.0\n\ndef lambda_to_C(lam, n): return 1.0 / (lam * n)\n\nLAM = 1e-2\nEPOCHS = 2000\nLR = 5e-2\n\nif DATASET == \"moons\":\n    X, y01 = make_moons(n_samples=N_SAMPLES, noise=NOISE, random_state=RS)\nelif DATASET == \"circles\":\n    X, y01 = make_circles(n_samples=N_SAMPLES, noise=NOISE, factor=0.5, random_state=RS)\nelse:\n    raise ValueError(\"DATASET must be 'moons' or 'circles'.\")\n\ny = 2*y01 - 1\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=TEST_SIZE, random_state=RS, stratify=y01)\n\nt0 = time()\ncustom = PolyKernelSVM_GD(\n    degree=DEGREE, gamma=GAMMA, coef0=COEF0,\n    lam=LAM, lr=LR, epochs=EPOCHS, verbose=False, random_state=RS\n).fit(X_tr, y_tr)\nt_custom = time() - t0\n\nC = lambda_to_C(LAM, n=X_tr.shape[0])\nt1 = time()\n\nsk = SVC(kernel=\"poly\", degree=DEGREE, gamma=GAMMA, coef0=COEF0, C=C)\nsk.fit(X_tr, (y_tr + 1) // 2)\nt_sk = time() - t1\n\ndef acc(model, X, y):\n    yp = model.predict(X)\n    if yp.min() >= 0:\n        yp = np.where(yp==1, 1, -1)\n    return (yp == y).mean()\n\ntrain_acc_custom = acc(custom, X_tr, y_tr)\ntest_acc_custom  = acc(custom, X_te, y_te)\n\ntrain_acc_sk = acc(sk, X_tr, y_tr)\ntest_acc_sk  = acc(sk, X_te, y_te)\n\nprint(f\"Dataset={DATASET}, degree={DEGREE}, gamma={GAMMA}, coef0={COEF0}\")\nprint(f\"Custom-GD:  lam={LAM}  epochs={EPOCHS}  lr={LR}  time={t_custom:.3f}s  \"\n      f\"train_acc={train_acc_custom:.3f}  test_acc={test_acc_custom:.3f}\")\nprint(f\"sklearn SVC: Câ‰ˆ{C:.4f} time={t_sk:.3f}s  \"\n      f\"train_acc={train_acc_sk:.3f}  test_acc={test_acc_sk:.3f}\")\n\nclass SKWrap:\n    def __init__(self, svc): self.svc = svc\n    def decision_function(self, Xq):\n        return self.svc.decision_function(Xq)\n    def predict(self, Xq):\n        return np.where(self.svc.decision_function(Xq) >= 0, 1, -1)\n\nplot_side_by_side(\n    [custom, SKWrap(sk)],\n    [f\"Custom Poly SVM (Î»={LAM})\", f\"sklearn SVC (Câ‰ˆ{C:.2g})\"],\n    X_tr, y_tr\n)",
   "metadata": {
    "id": "uLr18w6PULey"
   },
   "id": "uLr18w6PULey",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# **Questions and Answers** (10 points)\n\n---\n\n### **1. Regularization Parameter**  \n\nWhat role does the regularization parameter (C) play in a Linear SVM?\n\n**Answer:** The regularization parameter C controls the tradeoff between maximizing the margin and minimizing training errors. A larger C penalizes misclassifications more heavily, leading to a smaller margin but fewer training errors. A smaller C allows for more margin violations but creates a larger, more generalizable margin. Essentially, C balances model complexity and training accuracy.\n\n### **2. Linear SVM's**\n\nWhy are Linear SVMs typically preferred for high-dimensional, sparse datasets (like TF-IDF text features)?\n\n**Answer:** Linear SVMs are preferred for high-dimensional sparse data because they avoid expensive kernel computations and work efficiently when data is already linearly separable in high dimensions. They have fewer hyperparameters to tune, train faster, and are less prone to overfitting since they don't artificially increase dimensionality. For text data with thousands of features, linear models often perform as well as kernel methods while being much more computationally efficient.\n\n### **3. Polynomial SVM's**\nHow do the parameters degree (d), gamma (G), and coef0 (câ‚€) influence the decision boundary?\n\n**Answer:** The degree parameter d controls the complexity of polynomial interactions, with higher degrees creating more flexible and complex boundaries. Gamma G scales the dot product and controls how much influence individual training samples have; higher gamma makes the decision boundary more sensitive to nearby points and can lead to overfitting. Coef0 câ‚€ shifts the kernel and determines the importance of lower-order polynomial terms; higher coef0 gives more weight to constant and linear terms relative to higher-order interactions.\n\n### **4. Overfitting Polynomial SVM**\nWhy might a Polynomial SVM overfit more easily than a Linear SVM?\n\n**Answer:** Polynomial SVMs can overfit more easily because they have greater model capacity due to the implicit polynomial feature expansion. This increased flexibility allows the model to fit complex patterns in training data, including noise and outliers, rather than learning the true underlying pattern. The higher the polynomial degree, the more complex patterns the model can learn, increasing the risk of overfitting especially with limited training data.\n\n### **5. Polynomial Kernel Purpose**\nWhy does a Polynomial kernel make linear classifiers capable of separating nonlinear data?\n\n**Answer:** A Polynomial kernel implicitly maps data into a higher-dimensional feature space containing polynomial combinations of the original features. In this expanded space, relationships that were nonlinear in the original space become linear. The kernel trick allows computing these high-dimensional dot products efficiently without explicitly constructing the expanded features, enabling linear classifiers to learn nonlinear decision boundaries in the original space.",
   "metadata": {
    "id": "87m-wA1qlsAI"
   },
   "id": "87m-wA1qlsAI"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}